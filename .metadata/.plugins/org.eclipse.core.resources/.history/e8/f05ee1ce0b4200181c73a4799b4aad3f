/*
 * network.cpp
 *
 *  Created on: 23.11.2017
 *      Author:
 *
 *  This is the implementation of the Network class
 */

#include "Network.h"
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <math.h>
#include <limits>
#include <iostream>
#include <cublas_v2.h>
#include <cuda_runtime.h>
#include "cuda_kernels.h"



Network::Network()
{
	layer_list = new vector<Layer*>();

	nodeArrayPtrs = NULL;
	weightArrayPtrs = NULL;
	biasArrayPtrs = NULL;
	nodeDerivArrayPtrs = NULL;
	weightDerivArrayPtrs = NULL;
	biasDerivArrayPtrs = NULL;

	nodeArrayLengths = NULL;
	weightArrayLengths = NULL;
	biasArrayLengths = NULL;

	no_node_matrices = 0;
	no_weight_matrices = 0;
	no_bias_matrices = 0;

	train_picture_container = new PictureContainer("./train", 55);
	test_picture_container = new PictureContainer("./test", 10);
}

Network::~Network()
{
	cudaError_t cuda_error = cudaSuccess;

	for(unsigned int i = 0; i < layer_list->size(); i++)
	{
		delete layer_list->at(i);
	}


	delete layer_list;

	for(int i = 0; i < no_node_matrices; i++)
	{
		cuda_error = cudaFree(nodeArrayPtrs[i]);
		cuda_error = cudaFree(nodeDerivArrayPtrs[i]);
	}

	for(int i = 0; i < no_weight_matrices; i++)
	{
		cuda_error = cudaFree((void*)weightArrayPtrs[i]);
		cuda_error = cudaFree((void*)weightDerivArrayPtrs[i]);
	}

	for(int i = 0; i < no_bias_matrices; i++)
	{
		cuda_error = cudaFree((void*)biasArrayPtrs[i]);
		cuda_error = cudaFree((void*)biasDerivArrayPtrs[i]);
	}

	cuda_error = cudaFree((void**) nodeDeviceArrayLengths);
	cuda_error = cudaFree((void**) weightDeviceArrayLengths);
	cuda_error = cudaFree((void**) biasDeviceArrayLengths);
	cuda_error = cudaFree((void**) nodeDerivDeviceArrayLengths);
	cuda_error = cudaFree((void**) weightDerivDeviceArrayLengths);
	cuda_error = cudaFree((void**) biasDerivDeviceArrayLengths);

	cuda_error = cudaFree((void**) nodeDeviceMatrixDims_x);
	cuda_error = cudaFree((void**) weightDeviceMatrixDims_x);
	cuda_error = cudaFree((void**) biasDeviceMatrixDims_x);
	cuda_error = cudaFree((void**) nodeDerivDeviceMatrixDims_x);
	cuda_error = cudaFree((void**) weightDerivDeviceMatrixDims_x);
	cuda_error = cudaFree((void**) biasDerivDeviceMatrixDims_x);

	cuda_error = cudaFree((void**) nodeDeviceMatrixDims_y);
	cuda_error = cudaFree((void**) weightDeviceMatrixDims_y);
	cuda_error = cudaFree((void**) biasDeviceMatrixDims_y);
	cuda_error = cudaFree((void**) nodeDerivDeviceMatrixDims_y);
	cuda_error = cudaFree((void**) weightDerivDeviceMatrixDims_y);
	cuda_error = cudaFree((void**) biasDerivDeviceMatrixDims_y);

	free(nodeArrayPtrs);
	free(weightArrayPtrs);
	free(biasArrayPtrs);
	free(nodeDerivArrayPtrs);
	free(weightDerivArrayPtrs);
	free(biasArrayPtrs);

	free(nodeArrayLengths);
	free(weightArrayLengths);
	free(biasArrayLengths);

	free(nodeMatrixDims_x);
	free(weightMatrixDims_x);
	free(biasMatrixDims_x);

	free(nodeMatrixDims_y);
	free(weightMatrixDims_y);
	free(biasMatrixDims_y);
}

void Network::add_Layer(Layer* layer)
{
	layer_list->push_back(layer);
}

/**
 * This function sets up all Layers contained in layer_list as matrices
 * Make sure that your network is clear before you generate a new network
 *
 * <return> bool - indicates if initialization of the network was successful.
 * It can fail if your layers are not sorted in an expected manner </return>
 */
bool Network::generate_network()
{
	cudaError_t cuda_error = cudaSuccess;
	int node_index=0;
	int bias_index=0;
	int weight_index=0;

	/* transfer layer_list to GPU memory */
	Layer* layer_array = (Layer*) malloc(layer_list->size()*sizeof(Layer));
	cuda_error = cudaMalloc((void**) &device_layer_list, layer_list->size()*sizeof(Layer));


	for(unsigned int i = 0; i < layer_list->size(); i++)
	{
		Layer* layer = layer_list->at(i);
		layer_array[i] = *layer;
		switch(layer->getLayerType())
		{
			case INPUT_LAYER:
			{
				Input_Layer* input_layer = (Input_Layer*) layer;
				input_layer->setNodeIndex(node_index);
				/* allocate memory for pointers at host and memory for  node array *
				 * on GPU device
				 */
				nodeArrayPtrs = (float**) malloc(sizeof(float*));
				cuda_error = cudaMalloc((void**) &nodeArrayPtrs[0], input_layer->getRows()*input_layer->getCols() * sizeof(float));

				/* save array dimensions */
				nodeArrayLengths = (int*) malloc(sizeof(int));
				nodeMatrixDims_x = (int*) malloc(sizeof(int));
				nodeMatrixDims_y = (int*) malloc(sizeof(int));

				nodeArrayLengths[0] = input_layer->getRows()*input_layer->getCols();
				nodeMatrixDims_x[0] = input_layer->getRows();
				nodeMatrixDims_y[0] = input_layer->getCols();

				no_node_matrices++;
				node_index++;
				break;
			}
			case CONV_LAYER:
			{
				Conv_Layer* conv_layer = (Conv_Layer*) layer;
				conv_layer->setBiasIndex(bias_index);
				conv_layer->setNodeIndex(node_index);
				conv_layer->setWeightIndex(weight_index);
				if((layer_list->at(i-1)->getLayerType() == INPUT_LAYER))
				{
					Input_Layer* last_layer = (Input_Layer*) layer_list->at(i-1);
					int prev_dim_x = last_layer->getCols();
					int prev_dim_y = last_layer->getRows();

					int dim_x = (prev_dim_x - conv_layer->getXReceptive() + 1) / conv_layer->getStepSize();
					int dim_y = (prev_dim_y - conv_layer->getYReceptive() + 1) / conv_layer->getStepSize();

					conv_layer->setXSize(dim_x);
					conv_layer->setYSize(dim_y);

					/** adding a pointer to the pointer array be reallocating and resizing the array 		   *
					 * 	weight/bias array ptrs are not allocated yet, because previous layer was a input layer */
					nodeArrayPtrs   = (float**) realloc(nodeArrayPtrs, (node_index+1)*sizeof(float*));
					weightArrayPtrs = (float**) malloc(sizeof(float*));
					biasArrayPtrs   = (float**) malloc(sizeof(float*));
					weightDerivArrayPtrs = (float**) malloc(sizeof(float*));
					biasDerivArrayPtrs   = (float**) malloc(sizeof(float*));

					/** allocating memory on GPU device for the matrices */
					cuda_error = cudaMalloc((void**) &nodeArrayPtrs[node_index], dim_x*dim_y*conv_layer->getNoFeatureMaps()*sizeof(float));
					cuda_error = cudaMalloc((void**) &weightArrayPtrs[weight_index], conv_layer->getXReceptive()*conv_layer->getYReceptive()*
							conv_layer->getNoFeatureMaps()*sizeof(float));
					cuda_error = cudaMalloc((void**) &biasArrayPtrs[bias_index], dim_x*dim_y*conv_layer->getNoFeatureMaps()*sizeof(float));
					cuda_error = cudaMalloc((void**) &weightDerivArrayPtrs[weight_index], conv_layer->getXReceptive()*conv_layer->getYReceptive()*
							conv_layer->getNoFeatureMaps()*sizeof(float));
					cuda_error = cudaMalloc((void**) &biasDerivArrayPtrs[bias_index], dim_x*dim_y*conv_layer->getNoFeatureMaps()*sizeof(float));

					/* save array dimensions */
					nodeArrayLengths = (int*) realloc(nodeArrayLengths, (node_index+1)*sizeof(int));
					nodeMatrixDims_x = (int*) realloc(nodeMatrixDims_x, (node_index+1)*sizeof(int));
					nodeMatrixDims_y = (int*) realloc(nodeMatrixDims_y, (node_index+1)*sizeof(int));

					nodeArrayLengths[node_index] = dim_x * dim_y * conv_layer->getNoFeatureMaps();
					nodeMatrixDims_x[node_index] = dim_x * dim_y;
					nodeMatrixDims_y[node_index] = conv_layer->getNoFeatureMaps();

					weightArrayLengths = (int*) realloc(weightArrayLengths, (weight_index+1)*sizeof(int));
					weightMatrixDims_x = (int*) realloc(weightMatrixDims_x, (weight_index+1)*sizeof(int));
					weightMatrixDims_y = (int*) realloc(weightMatrixDims_y, (weight_index+1)*sizeof(int));

					weightArrayLengths[weight_index] = conv_layer->getXReceptive()*conv_layer->getYReceptive()*
															conv_layer->getNoFeatureMaps();
					weightMatrixDims_x[weight_index] = conv_layer->getXReceptive()*conv_layer->getYReceptive();
					weightMatrixDims_y[weight_index] = conv_layer->getNoFeatureMaps();


					biasArrayLengths = (int*) realloc(biasArrayLengths, (bias_index+1)*sizeof(int));
					biasMatrixDims_x = (int*) realloc(biasMatrixDims_x, (bias_index+1)*sizeof(int));
					biasMatrixDims_y = (int*) realloc(biasMatrixDims_y, (bias_index+1)*sizeof(int));

					biasArrayLengths[bias_index] = dim_x * dim_y * conv_layer->getNoFeatureMaps();
					biasMatrixDims_x[bias_index] = dim_x * dim_y;
					biasMatrixDims_y[bias_index] = conv_layer->getNoFeatureMaps();


					weight_index++;
					bias_index++;
					node_index++;
					no_node_matrices++;
					no_weight_matrices++;
					no_bias_matrices++;

					conv_layer->setSize(conv_layer->getNoFeatureMaps()*dim_x*dim_y);
				}
				else if((layer_list->at(i-1)->getLayerType() == POOLING_LAYER))
				{
					MaxPooling_Layer* last_layer = (MaxPooling_Layer*) (layer_list->at(i-1));

					int prev_dim_x = last_layer->getXSize();
					int prev_dim_y = last_layer->getYSize();
					int prev_no_features = last_layer->getNoFeatures();

					int dim_x = (prev_dim_x - conv_layer->getXReceptive() + 1) / conv_layer->getStepSize();
					int dim_y = (prev_dim_y - conv_layer->getYReceptive() + 1) / conv_layer->getStepSize();

					conv_layer->setXSize(dim_x);
					conv_layer->setYSize(dim_y);

					/** adding a pointer to the pointer arrays be reallocating and resizing the arrays 		   */
					nodeArrayPtrs   = (float**) realloc(nodeArrayPtrs, (node_index+1)*sizeof(float*));
					weightArrayPtrs = (float**) realloc(weightArrayPtrs, (weight_index+1)*sizeof(float*));
					biasArrayPtrs   = (float**) realloc(biasArrayPtrs, (bias_index+1)*sizeof(float*));
					weightDerivArrayPtrs = (float**) realloc(weightDerivArrayPtrs, (weight_index+1)*sizeof(float*));
					biasDerivArrayPtrs   = (float**) realloc(biasDerivArrayPtrs, (bias_index+1)*sizeof(float*));

					/** allocating memory on GPU device for the matrices */
					cuda_error = cudaMalloc((void**) &nodeArrayPtrs[node_index], dim_x*dim_y*conv_layer->getNoFeatureMaps()*sizeof(float));
					cuda_error = cudaMalloc((void**) &weightArrayPtrs[weight_index], conv_layer->getXReceptive()*conv_layer->getYReceptive()*
							prev_no_features * conv_layer->getNoFeatureMaps()*sizeof(float));
					cuda_error = cudaMalloc((void**) &biasArrayPtrs[bias_index], dim_x*dim_y*conv_layer->getNoFeatureMaps()*sizeof(float));
					cuda_error = cudaMalloc((void**) &weightDerivArrayPtrs[weight_index], conv_layer->getXReceptive()*conv_layer->getYReceptive()*
							prev_no_features * conv_layer->getNoFeatureMaps()*sizeof(float));
					cuda_error = cudaMalloc((void**) &biasDerivArrayPtrs[bias_index], dim_x*dim_y*conv_layer->getNoFeatureMaps()*sizeof(float));


					/* save array dimensions */
					nodeArrayLengths = (int*) realloc(nodeArrayLengths, (node_index+1)*sizeof(int));
					nodeMatrixDims_x = (int*) realloc(nodeMatrixDims_x, (node_index+1)*sizeof(int));
					nodeMatrixDims_y = (int*) realloc(nodeMatrixDims_y, (node_index+1)*sizeof(int));

					nodeArrayLengths[node_index] = dim_x * dim_y * conv_layer->getNoFeatureMaps();
					nodeMatrixDims_x[node_index] = dim_x * dim_y;
					nodeMatrixDims_y[node_index] = conv_layer->getNoFeatureMaps();

					weightArrayLengths = (int*) realloc(weightArrayLengths, (weight_index+1)*sizeof(int));
					weightMatrixDims_x = (int*) realloc(weightMatrixDims_x, (weight_index+1)*sizeof(int));
					weightMatrixDims_y = (int*) realloc(weightMatrixDims_y, (weight_index+1)*sizeof(int));

					weightArrayLengths[weight_index] = conv_layer->getXReceptive()*conv_layer->getYReceptive()*
														prev_no_features * conv_layer->getNoFeatureMaps();
					weightMatrixDims_x[weight_index] = conv_layer->getXReceptive()*conv_layer->getYReceptive()*prev_no_features;
					weightMatrixDims_y[weight_index] = conv_layer->getNoFeatureMaps();


					biasArrayLengths = (int*) realloc(biasArrayLengths, (bias_index+1)*sizeof(int));
					biasMatrixDims_x = (int*) realloc(biasMatrixDims_x, (bias_index+1)*sizeof(int));
					biasMatrixDims_y = (int*) realloc(biasMatrixDims_y, (bias_index+1)*sizeof(int));

					biasArrayLengths[bias_index] = dim_x * dim_y * conv_layer->getNoFeatureMaps();
					biasMatrixDims_x[bias_index] = dim_x * dim_y;
					biasMatrixDims_y[bias_index] = conv_layer->getNoFeatureMaps();

					node_index++;
					weight_index++;
					bias_index++;

					no_node_matrices++;
					no_weight_matrices++;
					no_bias_matrices++;

					conv_layer->setSize(conv_layer->getNoFeatureMaps()*dim_x*dim_y);
				}
				else
				{
					return false;
				}
				break;
			}
			case POOLING_LAYER:
			{
				MaxPooling_Layer* pooling_layer = (MaxPooling_Layer*) layer;
				pooling_layer->setNodeIndex(node_index);
				if((layer_list->at(i-1)->getLayerType() == CONV_LAYER))
				{
					Conv_Layer* last_layer = (Conv_Layer*) layer_list->at(i-1);
					int prev_no_features = last_layer->getNoFeatureMaps();
					int prev_dim_x = last_layer->getXSize();
					int prev_dim_y = last_layer->getYSize();
					int pool_dim_x = (prev_dim_x / pooling_layer->getXReceptive());
					int pool_dim_y = (prev_dim_y / pooling_layer->getYReceptive());
					int new_no_cols, new_no_rows;

					pooling_layer->setXSize(pool_dim_x);
					pooling_layer->setYSize(pool_dim_y);
					pooling_layer->setSize(pool_dim_x*pool_dim_y);

					if(layer_list->at(i+1)->getLayerType() == CONV_LAYER)
					{
						Conv_Layer *next_layer = (Conv_Layer*) layer_list->at(i+1);

						int dim_x = (pool_dim_x - next_layer->getXReceptive() + 1) / next_layer->getStepSize();
						int dim_y = (pool_dim_y - next_layer->getYReceptive() + 1) / next_layer->getStepSize();

						new_no_cols = next_layer->getXReceptive()*next_layer->getYReceptive()*last_layer->getNoFeatureMaps();
						new_no_rows = dim_x*dim_y;
					}
					else if (layer_list->at(i+1)->getLayerType() == FULLY_CONNECTED_LAYER)
					{
						new_no_cols = (last_layer->getXSize()/pooling_layer->getXReceptive()) * (last_layer->getYSize()/pooling_layer->getYReceptive()) *
								last_layer->getNoFeatureMaps();
						new_no_rows = 1;
					}

					/** adding a pointer to the pointer array be reallocating and resizing the array 		   */
					nodeArrayPtrs = (float**) realloc(nodeArrayPtrs, (node_index+1) * sizeof(float*));
					cuda_error = cudaMalloc((void**) &nodeArrayPtrs[node_index], new_no_rows * new_no_cols * sizeof(float));

					/* save array dimensions */
					nodeArrayLengths = (int*) realloc(nodeArrayLengths, (node_index+1)*sizeof(int));
					nodeMatrixDims_x = (int*) realloc(nodeMatrixDims_x, (node_index+1)*sizeof(int));
					nodeMatrixDims_y = (int*) realloc(nodeMatrixDims_y, (node_index+1)*sizeof(int));

					nodeArrayLengths[node_index] = new_no_rows * new_no_cols;
					nodeMatrixDims_x[node_index] = new_no_rows;
					nodeMatrixDims_y[node_index] = new_no_cols;



					node_index++;
					no_node_matrices++;

					pooling_layer->setSize(pool_dim_x*pool_dim_y*prev_no_features);
				}
				else
				{
					return false;
				}
				break;
			}
			case FULLY_CONNECTED_LAYER:
			{
				FullyConnected_Layer* fullyConn_layer = (FullyConnected_Layer*) layer;
				fullyConn_layer->setBiasIndex(bias_index);
				fullyConn_layer->setNodeIndex(node_index);
				fullyConn_layer->setWeightIndex(weight_index);

				/** adding a pointer to the pointer arrays be reallocating and resizing the arrays 		   */
				nodeArrayPtrs   = (float**) realloc(nodeArrayPtrs, (node_index+1)*sizeof(float*));
				weightArrayPtrs = (float**) realloc(weightArrayPtrs, (weight_index+1)*sizeof(float*));
				biasArrayPtrs   = (float**) realloc(biasArrayPtrs, (bias_index+1)*sizeof(float*));
				weightDerivArrayPtrs = (float**) realloc(weightDerivArrayPtrs, (weight_index+1)*sizeof(float*));
				biasDerivArrayPtrs   = (float**) realloc(biasDerivArrayPtrs, (bias_index+1)*sizeof(float*));

				/** allocating memory on GPU device for the matrices */
				cuda_error = cudaMalloc((void**) &nodeArrayPtrs[node_index], fullyConn_layer->getSize()*sizeof(float));
				cuda_error = cudaMalloc((void**) &weightArrayPtrs[weight_index], layer_list->at(i-1)->getSize()*fullyConn_layer->getSize()*sizeof(float));
				cuda_error = cudaMalloc((void**) &biasArrayPtrs[bias_index], fullyConn_layer->getSize()*sizeof(float));
				cuda_error = cudaMalloc((void**) &weightDerivArrayPtrs[weight_index], layer_list->at(i-1)->getSize()*fullyConn_layer->getSize()*sizeof(float));
				cuda_error = cudaMalloc((void**) &biasDerivArrayPtrs[bias_index], fullyConn_layer->getSize()*sizeof(float));

				/* save array dimensions */
				nodeArrayLengths = (int*) realloc(nodeArrayLengths, (node_index+1)*sizeof(int));
				nodeMatrixDims_x = (int*) realloc(nodeMatrixDims_x, (node_index+1)*sizeof(int));
				nodeMatrixDims_y = (int*) realloc(nodeMatrixDims_y, (node_index+1)*sizeof(int));

				nodeArrayLengths[node_index] = fullyConn_layer->getSize();
				nodeMatrixDims_x[node_index] = 1;
				nodeMatrixDims_y[node_index] = fullyConn_layer->getSize();

				weightArrayLengths = (int*) realloc(weightArrayLengths, (weight_index+1)*sizeof(int));
				weightMatrixDims_x = (int*) realloc(weightMatrixDims_x, (weight_index+1)*sizeof(int));
				weightMatrixDims_y = (int*) realloc(weightMatrixDims_y, (weight_index+1)*sizeof(int));

				weightArrayLengths[weight_index] = layer_list->at(i-1)->getSize()*fullyConn_layer->getSize();
				weightMatrixDims_x[weight_index] = layer_list->at(i-1)->getSize();
				weightMatrixDims_y[weight_index] = fullyConn_layer->getSize();


				biasArrayLengths = (int*) realloc(biasArrayLengths, (bias_index+1)*sizeof(int));
				biasMatrixDims_x = (int*) realloc(biasMatrixDims_x, (bias_index+1)*sizeof(int));
				biasMatrixDims_y = (int*) realloc(biasMatrixDims_y, (bias_index+1)*sizeof(int));

				biasArrayLengths[bias_index] = fullyConn_layer->getSize();
				biasMatrixDims_x[bias_index] = 1;
				biasMatrixDims_y[bias_index] = fullyConn_layer->getSize();

				node_index++;
				weight_index++;
				bias_index++;

				no_node_matrices++;
				no_weight_matrices++;
				no_bias_matrices++;

				break;
			}
			case DROPOUT_LAYER:
			{
				//TODO not implemented yet
				break;
			}
			default:
			{
				break;
			}
		}
	}

	/* allocate memory on GPU device for network matrix array pointers */

	cuda_error = cudaMalloc((void**) &nodeDeviceArrayPtrs, no_node_matrices*sizeof(float*));
	cuda_error = cudaMalloc((void**) &weightDeviceArrayPtrs, no_weight_matrices*sizeof(float*));
	cuda_error = cudaMalloc((void**) &biasDeviceArrayPtrs, no_bias_matrices*sizeof(float*));
//	cuda_error = cudaMalloc((void**) &nodeDerivDeviceArrayPtrs, no_node_matrices*sizeof(float*));
	cuda_error = cudaMalloc((void**) &weightDerivDeviceArrayPtrs, no_weight_matrices*sizeof(float*));
	cuda_error = cudaMalloc((void**) &biasDerivDeviceArrayPtrs, no_bias_matrices*sizeof(float*));

	/* copy pointers to arrays to GPU memory */

	cuda_error = cudaMemcpy(nodeDeviceArrayPtrs, nodeArrayPtrs, no_node_matrices*sizeof(float*), cudaMemcpyHostToDevice);
	cuda_error = cudaMemcpy(weightDeviceArrayPtrs, weightArrayPtrs, no_weight_matrices*sizeof(float*), cudaMemcpyHostToDevice);
	cuda_error = cudaMemcpy(biasDeviceArrayPtrs, biasArrayPtrs, no_bias_matrices*sizeof(float*), cudaMemcpyHostToDevice);
//	cuda_error = cudaMemcpy(nodeDerivDeviceArrayPtrs, nodeDerivArrayPtrs, no_node_matrices*sizeof(float*), cudaMemcpyHostToDevice);
	cuda_error = cudaMemcpy(weightDerivDeviceArrayPtrs, weightDerivArrayPtrs, no_weight_matrices*sizeof(float*), cudaMemcpyHostToDevice);
	cuda_error = cudaMemcpy(biasDerivDeviceArrayPtrs, biasDerivArrayPtrs, no_bias_matrices*sizeof(float*), cudaMemcpyHostToDevice);


	/* allocate memory on GPU device for array lengths and matrix dimensions */

	cuda_error = cudaMalloc((void**) &nodeDeviceArrayLengths, no_node_matrices * sizeof(int));
	cuda_error = cudaMalloc((void**) &weightDeviceArrayLengths, no_weight_matrices * sizeof(int));
	cuda_error = cudaMalloc((void**) &biasDeviceArrayLengths, no_bias_matrices * sizeof(int));
	cuda_error = cudaMalloc((void**) &nodeDerivDeviceArrayLengths, no_node_matrices * sizeof(int));
	cuda_error = cudaMalloc((void**) &weightDerivDeviceArrayLengths, no_weight_matrices * sizeof(int));
	cuda_error = cudaMalloc((void**) &biasDerivDeviceArrayLengths, no_bias_matrices * sizeof(int));

	cuda_error = cudaMalloc((void**) &nodeDeviceMatrixDims_x, no_node_matrices * sizeof(int));
	cuda_error = cudaMalloc((void**) &weightDeviceMatrixDims_x, no_weight_matrices * sizeof(int));
	cuda_error = cudaMalloc((void**) &biasDeviceMatrixDims_x, no_bias_matrices * sizeof(int));
	cuda_error = cudaMalloc((void**) &nodeDerivDeviceMatrixDims_x, no_node_matrices * sizeof(int));
	cuda_error = cudaMalloc((void**) &weightDerivDeviceMatrixDims_x, no_weight_matrices * sizeof(int));
	cuda_error = cudaMalloc((void**) &biasDerivDeviceMatrixDims_x, no_bias_matrices * sizeof(int));

	cuda_error = cudaMalloc((void**) &nodeDeviceMatrixDims_y, no_node_matrices * sizeof(int));
	cuda_error = cudaMalloc((void**) &weightDeviceMatrixDims_y, no_weight_matrices * sizeof(int));
	cuda_error = cudaMalloc((void**) &biasDeviceMatrixDims_y, no_bias_matrices * sizeof(int));
	cuda_error = cudaMalloc((void**) &nodeDerivDeviceMatrixDims_y, no_node_matrices * sizeof(int));
	cuda_error = cudaMalloc((void**) &weightDerivDeviceMatrixDims_y, no_weight_matrices * sizeof(int));
	cuda_error = cudaMalloc((void**) &biasDerivDeviceMatrixDims_y, no_bias_matrices * sizeof(int));

	/* transfer host data to device memory */

	cuda_error = cudaMemcpy(nodeDeviceArrayLengths, nodeArrayLengths, no_node_matrices * sizeof(int), cudaMemcpyHostToDevice);
	cuda_error = cudaMemcpy(weightDeviceArrayLengths, weightArrayLengths, no_weight_matrices * sizeof(int), cudaMemcpyHostToDevice);
	cuda_error = cudaMemcpy(biasDeviceArrayLengths, biasArrayLengths, no_bias_matrices * sizeof(int), cudaMemcpyHostToDevice);
	cuda_error = cudaMemcpy(nodeDerivDeviceArrayLengths, nodeArrayLengths, no_node_matrices * sizeof(int), cudaMemcpyHostToDevice);
	cuda_error = cudaMemcpy(weightDerivDeviceArrayLengths, weightArrayLengths, no_weight_matrices * sizeof(int), cudaMemcpyHostToDevice);
	cuda_error = cudaMemcpy(biasDerivDeviceArrayLengths, biasArrayLengths, no_bias_matrices * sizeof(int), cudaMemcpyHostToDevice);

	cuda_error = cudaMemcpy(nodeDeviceMatrixDims_x, nodeMatrixDims_x, no_node_matrices * sizeof(int), cudaMemcpyHostToDevice);
	cuda_error = cudaMemcpy(weightDeviceMatrixDims_x, weightMatrixDims_x, no_weight_matrices * sizeof(int), cudaMemcpyHostToDevice);
	cuda_error = cudaMemcpy(biasDeviceMatrixDims_x, biasMatrixDims_x, no_bias_matrices * sizeof(int), cudaMemcpyHostToDevice);
	cuda_error = cudaMemcpy(nodeDerivDeviceMatrixDims_x, nodeMatrixDims_x, no_node_matrices * sizeof(int), cudaMemcpyHostToDevice);
	cuda_error = cudaMemcpy(weightDerivDeviceMatrixDims_x, weightMatrixDims_x, no_weight_matrices * sizeof(int), cudaMemcpyHostToDevice);
	cuda_error = cudaMemcpy(biasDerivDeviceMatrixDims_x, biasMatrixDims_x, no_bias_matrices * sizeof(int), cudaMemcpyHostToDevice);

	cuda_error = cudaMemcpy(nodeDeviceMatrixDims_y, nodeMatrixDims_y, no_node_matrices * sizeof(int), cudaMemcpyHostToDevice);
	cuda_error = cudaMemcpy(weightDeviceMatrixDims_y, weightMatrixDims_y, no_weight_matrices * sizeof(int), cudaMemcpyHostToDevice);
	cuda_error = cudaMemcpy(biasDeviceMatrixDims_y, biasMatrixDims_y, no_bias_matrices * sizeof(int), cudaMemcpyHostToDevice);
	cuda_error = cudaMemcpy(nodeDerivDeviceMatrixDims_y, nodeMatrixDims_y, no_node_matrices * sizeof(int), cudaMemcpyHostToDevice);
	cuda_error = cudaMemcpy(weightDerivDeviceMatrixDims_y, weightMatrixDims_y, no_weight_matrices * sizeof(int), cudaMemcpyHostToDevice);
	cuda_error = cudaMemcpy(biasDerivDeviceMatrixDims_y, biasMatrixDims_y, no_bias_matrices * sizeof(int), cudaMemcpyHostToDevice);

	cuda_error = cudaMemcpy(device_layer_list, layer_array, layer_list->size()*sizeof(Layer), cudaMemcpyHostToDevice);

	/* initializes matrices with uniform pseudo-random values between 0.0 and 1.0 */
	cuda::init<<<1,80>>>(nodeDeviceArrayPtrs, no_node_matrices, nodeDeviceArrayLengths);
	cuda::init<<<1,80>>>(weightDeviceArrayPtrs, no_weight_matrices, weightDeviceArrayLengths);
	cuda::init<<<1,80>>>(biasDeviceArrayPtrs, no_bias_matrices, biasDeviceArrayLengths);

//	cudaDeviceSynchronize();
//
//	for(int i = 0; i < no_node_matrices; i++)
//	{
//		cuda::printMatrix<<<1,1>>>(nodeArrayPtrs[i], nodeMatrixDims_x[i], nodeMatrixDims_y[i]);
//		printf("\n\n");
//	}
//	cudaDeviceSynchronize();
//	for(int i = 0; i < no_weight_matrices; i++)
//	{
//		cuda::printMatrix<<<1,1>>>(weightArrayPtrs[i], weightMatrixDims_x[i], weightMatrixDims_y[i]);
//		printf("\n\n");
//		cuda::printMatrix<<<1,1>>>(biasArrayPtrs[i], biasMatrixDims_x[i], biasMatrixDims_y[i]);
//		printf("\n\n");
//	}

	cudaDeviceSynchronize();

	return true;
}

bool Network::train(int batch_size, int no_iterations)
{
	cudaError_t cuda_error = cudaSuccess;
	float* devicePictureAddr, *deviceLabelAddr;
	int outer_loop = NO_DATA_D/batch_size;

	bool ret_val = false;

	std::cout << "Copying network for training..." << std::endl;

	float*** nodeArrays_3, ***weightArrays_3, ***biasArrays_3, ***nodeDerivArrays_3, ***weightDerivArrays_3;

	float*** nodeDeviceArrays_3, ***weightDeviceArrays_3, ***biasDeviceArrays_3, ***nodeDerivDeviceArrays_3, ***weightDerivDeviceArrays_3;
	float** nodeDeviceArrays_2, **weightDeviceArrays_2, **biasDeviceArrays_2, **nodeDerivDeviceArrays_2, **weightDerivDeviceArrays_2;

	nodeArrays_3 = (float***) malloc(batch_size*sizeof(float**));
	weightArrays_3 = (float***) malloc(batch_size*sizeof(float**));
	biasArrays_3 = (float***) malloc(batch_size*sizeof(float**));
	nodeDerivArrays_3 = (float***) malloc(batch_size*sizeof(float**));
	weightDerivArrays_3 = (float***) malloc(batch_size*sizeof(float**));

	cuda_error = cudaMalloc((void**) &nodeDeviceArrays_3, batch_size*sizeof(float**));
	cuda_error = cudaMalloc((void**) &weightDeviceArrays_3, batch_size*sizeof(float**));
	cuda_error = cudaMalloc((void**) &biasDeviceArrays_3, batch_size*sizeof(float**));
	cuda_error = cudaMalloc((void**) &nodeDerivDeviceArrays_3, batch_size*sizeof(float**));
	cuda_error = cudaMalloc((void**) &weightDerivDeviceArrays_3, batch_size*sizeof(float**));


	for(int i = 0; i < batch_size; i++)
	{
		/* copying nodes, weights and biases for each picture-parallel thread to prevent race conditions */

		nodeArrays_3[i] = (float**) malloc(no_node_matrices*sizeof(float*));
		weightArrays_3[i] = (float**) malloc(no_weight_matrices*sizeof(float*));
		biasArrays_3[i] = (float**) malloc(no_bias_matrices*sizeof(float*));
		nodeDerivArrays_3[i] = (float**) malloc(no_node_matrices*sizeof(float*));
		weightDerivArrays_3[i] = (float**) malloc(no_weight_matrices*sizeof(float*));

		cuda_error = cudaMalloc((void**) &nodeDeviceArrays_2, no_node_matrices*sizeof(float*));
		cuda_error = cudaMalloc((void**) &weightDeviceArrays_2, no_weight_matrices*sizeof(float*));
		cuda_error = cudaMalloc((void**) &biasDeviceArrays_2, no_bias_matrices*sizeof(float*));
		cuda_error = cudaMalloc((void**) &nodeDerivDeviceArrays_2, no_node_matrices*sizeof(float*));
		cuda_error = cudaMalloc((void**) &weightDerivDeviceArrays_2, no_weight_matrices*sizeof(float*));


		for(int j = 0; j < no_node_matrices; j++)
		{
			cuda_error = cudaMalloc((void**) &nodeArrays_3[i][j], nodeMatrixDims_x[j]*nodeMatrixDims_y[j]*sizeof(float));
			cuda_error = cudaMalloc((void**) &nodeDerivArrays_3[i][j], nodeMatrixDims_x[j]*nodeMatrixDims_y[j]*sizeof(float));
		}

		for(int j = 0; j < no_weight_matrices; j++)
		{
			cuda_error = cudaMalloc((void**) &weightArrays_3[i][j], weightMatrixDims_x[j]*weightMatrixDims_y[j]*sizeof(float));

			cuda_error = cudaMalloc((void**) &weightDerivArrays_3[i][j], weightMatrixDims_x[j]*weightMatrixDims_y[j]*sizeof(float));

			cuda_error = cudaMalloc((void**) &biasArrays_3[i][j], biasMatrixDims_x[j]*biasMatrixDims_y[j]*sizeof(float));

		}

		cuda_error = cudaMemcpy(nodeDeviceArrays_2, nodeArrays_3[i], no_node_matrices*sizeof(float*), cudaMemcpyHostToDevice);
		cuda_error = cudaMemcpy(&nodeDeviceArrays_3[i], &nodeDeviceArrays_2, sizeof(float**), cudaMemcpyHostToDevice);
		cuda_error = cudaMemcpy(weightDeviceArrays_2, weightArrays_3[i], no_weight_matrices*sizeof(float*), cudaMemcpyHostToDevice);
		cuda_error = cudaMemcpy(&weightDeviceArrays_3[i], &weightDeviceArrays_2, sizeof(float**), cudaMemcpyHostToDevice);
		cuda_error = cudaMemcpy(biasDeviceArrays_2, biasArrays_3[i], no_weight_matrices*sizeof(float*), cudaMemcpyHostToDevice);
		cuda_error = cudaMemcpy(&biasDeviceArrays_3[i], &biasDeviceArrays_2, sizeof(float**), cudaMemcpyHostToDevice);
		cuda_error = cudaMemcpy(weightDerivDeviceArrays_2, weightDerivArrays_3[i], no_weight_matrices*sizeof(float*), cudaMemcpyHostToDevice);
		cuda_error = cudaMemcpy(&weightDerivDeviceArrays_3[i], &weightDerivDeviceArrays_2, sizeof(float**), cudaMemcpyHostToDevice);
		cuda_error = cudaMemcpy(nodeDerivDeviceArrays_2, nodeDerivArrays_3[i], no_node_matrices*sizeof(float*), cudaMemcpyHostToDevice);
		cuda_error = cudaMemcpy(&nodeDerivDeviceArrays_3[i], &nodeDerivDeviceArrays_2, sizeof(float**), cudaMemcpyHostToDevice);
	}



	/* allocate device memory for a batch of input pictures */
	cuda_error = cudaMalloc((void**) &devicePictureAddr, batch_size * 784 * sizeof(float));
	cuda_error = cudaMalloc((void**) &deviceLabelAddr, batch_size * 10 * sizeof(float));

	for(int l = 0; l < no_iterations; l++)
	{
		for(int j = 0; j < outer_loop; j++)
		{
			/* copy weights and biases */
			for(int i = 0; i < batch_size; i++)
			{
				for(int j = 0; j < no_weight_matrices; j++)
				{
					cuda_error = cudaMemcpy((void*) weightArrays_3[i][j], (void*) weightArrayPtrs[j], weightMatrixDims_x[j]*weightMatrixDims_y[j]*sizeof(float), cudaMemcpyDeviceToDevice);
					cuda::print(weightArrays_3[i][j], weightMatrixDims_x[j], weightMatrixDims_y[j]);
					cuda_error = cudaMemcpy((void*) biasArrays_3[i][j], (void*) biasArrayPtrs[j], biasMatrixDims_x[j]*biasMatrixDims_y[j]*sizeof(float), cudaMemcpyDeviceToDevice);
					cuda::print(biasArrays_3[i][j], biasMatrixDims_x[j], biasMatrixDims_x[j]);
					cudaDeviceSynchronize();
				}
			}

			/* transfer picture data to device */
			for(int i = 0; i < batch_size; i++)
			{
				Picture* picture = train_picture_container->get_nextpicture();

				cuda_error = cudaMemcpy(&devicePictureAddr[i*784], picture->get_input(), 784 * sizeof(float), cudaMemcpyHostToDevice);
				cuda_error = cudaMemcpy(&deviceLabelAddr[i*10], picture->get_output(), 10 * sizeof(float), cudaMemcpyHostToDevice);


			}
			cuda::train<<<1,150>>>(device_layer_list, layer_list->size(), devicePictureAddr, batch_size, deviceLabelAddr,
					nodeDeviceArrays_3, weightDeviceArrays_3, biasDeviceArrays_3, nodeDerivDeviceArrays_3, weightDerivDeviceArrays_3,
					weightDeviceArrayPtrs, biasDeviceArrayPtrs, weightDerivDeviceArrayPtrs, biasDerivDeviceArrayPtrs,
					no_node_matrices, no_weight_matrices, no_bias_matrices,
					nodeDeviceMatrixDims_x, nodeDeviceMatrixDims_y, weightDeviceMatrixDims_x,
					weightDeviceMatrixDims_y, biasDeviceMatrixDims_x, biasDeviceMatrixDims_y);
		}
	}

	cuda_error = cudaFree((void*) devicePictureAddr);
	cuda_error = cudaFree((void*) deviceLabelAddr);

	for(int i = 0; i < batch_size; i++)
	{
		/* copying nodes, weights and biases for each picture-parallel thread to prevent race conditions */

		for(int j = 0; j < no_node_matrices; j++)
		{
			cuda_error = cudaFree((void*) nodeArrays_3[i][j]);
			cuda_error = cudaFree((void*) nodeDerivArrays_3[i][j]);
		}

		for(int j = 0; j < no_weight_matrices; j++)
		{
			cuda_error = cudaFree((void*) weightArrays_3[i][j]);

			cuda_error = cudaFree((void*) weightDerivArrays_3[i][j]);

			cuda_error = cudaFree((void*) biasArrays_3[i][j]);

		}

		free(nodeArrays_3[i]);
		free(weightArrays_3[i]);
		free(biasArrays_3[i]);
		free(nodeDerivArrays_3[i]);
		free(weightDerivArrays_3[i]);
	}

	cuda_error = cudaFree((void*) nodeDeviceArrays_2);
	cuda_error = cudaFree((void*) weightDeviceArrays_2);
	cuda_error = cudaFree((void*) biasDeviceArrays_2);
	cuda_error = cudaFree((void*) nodeDerivDeviceArrays_2);
	cuda_error = cudaFree((void*) weightDerivDeviceArrays_2);

	cuda_error = cudaFree((void*) nodeDeviceArrays_3);
	cuda_error = cudaFree((void*) weightDeviceArrays_3);
	cuda_error = cudaFree((void*) biasDeviceArrays_3);
	cuda_error = cudaFree((void*) nodeDerivDeviceArrays_3);
	cuda_error = cudaFree((void*) weightDerivDeviceArrays_3);

	free(nodeArrays_3);
	free(weightArrays_3);
	free(biasArrays_3);
	free(nodeDerivArrays_3);
	free(weightDerivArrays_3);

	return ret_val;
}

float Network::test()
{
	cudaError_t cuda_error = cudaSuccess;
	int batch_size = BATCH_SIZE;
	int correct_detections = 0;
	int correct_per_batch = 0;
	float* devicePictureAddr, *deviceLabelAddr;

	float*** nodeArrays, ***weightArrays, ***biasArrays;

	cuda_error = cudaMalloc((void**) &nodeArrays, batch_size*sizeof(float**));
	cuda_error = cudaMalloc((void**) &weightArrays, batch_size*sizeof(float**));
	cuda_error = cudaMalloc((void**) &biasArrays, batch_size*sizeof(float**));

	for(int i = 0; i < batch_size; i++)
	{
		/* copying nodes, weights and biases for each picture-parallel thread to prevent race conditions */

		cuda_error = cudaMalloc((void**) &nodeArrays[i], no_node_matrices*sizeof(float*));
		cuda_error = cudaMalloc((void**) &weightArrays[i], no_weight_matrices*sizeof(float*));
		cuda_error = cudaMalloc((void**) &biasArrays[i], no_bias_matrices*sizeof(float*));

		for(int j = 0; j < no_node_matrices; j++)
		{
			cuda_error = cudaMalloc((void**) &nodeArrays[i][j], nodeMatrixDims_x[j]*nodeMatrixDims_y[j]*sizeof(float));
		}

		for(int j = 0; j < no_weight_matrices; j++)
		{
			cuda_error = cudaMalloc((void**) &weightArrays[i][j], weightMatrixDims_x[j]*weightMatrixDims_y[j]*sizeof(float));
			cuda_error = cudaMalloc((void**) &biasArrays[i][j], biasMatrixDims_x[j]*biasMatrixDims_y[j]*sizeof(float));

		}
	}

	/* allocate device memory for a batch of input pictures */
	cuda_error = cudaMalloc((void**) &devicePictureAddr, batch_size * 784 * sizeof(float));

	for(int i = 0; i < NO_TEST_FILES_D; i++)
	{
		for(int j = 0; j < NO_PICS_PER_FILE_D; j++)
		{
			/* transfer picture data to device */
			for(int i = 0; i < batch_size; i++)
			{
				Picture* picture = train_picture_container->get_nextpicture();

				cuda_error = cudaMemcpy(&devicePictureAddr[i*784], picture->get_input(), 784 * sizeof(float), cudaMemcpyHostToDevice);

			}

			cuda::test<<<1,150>>>(device_layer_list, layer_list->size(), devicePictureAddr, batch_size, deviceLabelAddr,
					nodeArrays, weightArrays, biasArrays, no_node_matrices, no_weight_matrices, no_bias_matrices, nodeDeviceMatrixDims_x,
					nodeDeviceMatrixDims_y, weightDeviceMatrixDims_x,
					weightDeviceMatrixDims_y, biasDeviceMatrixDims_x, biasDeviceMatrixDims_y, &correct_per_batch);
			correct_detections += correct_per_batch;
		}
	}

	cuda_error = cudaFree((void*) devicePictureAddr);
	cuda_error = cudaFree((void*) deviceLabelAddr);

	for(int i = 0; i < batch_size; i++)
	{
		/* copying nodes, weights and biases for each picture-parallel thread to prevent race conditions */

		for(int j = 0; j < no_node_matrices; j++)
		{
			cuda_error = cudaFree((void*) nodeArrays[i][j]);
		}

		for(int j = 0; j < no_weight_matrices; j++)
		{
			cuda_error = cudaFree((void*) weightArrays[i][j]);
			cuda_error = cudaFree((void*) biasArrays[i][j]);

		}

		cuda_error = cudaFree((void*) nodeArrays[i]);
		cuda_error = cudaFree((void*) weightArrays[i]);
		cuda_error = cudaFree((void*) biasArrays[i]);
	}

	return (float)correct_detections/(NO_PICS_PER_FILE_D*NO_TEST_FILES_D);
}

